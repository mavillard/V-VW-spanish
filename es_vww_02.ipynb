{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/stories.pickle', 'rb') as f:\n",
    "    stories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/aux/es_lexicon.csv') as f:\n",
    "    reader = csv.reader(\n",
    "        f,\n",
    "        delimiter=' ',\n",
    "    )\n",
    "    lexicon = []\n",
    "    for row in reader:\n",
    "        for i in range(1, len(row[1:]), 2):\n",
    "            entry = {}\n",
    "            entry['flexion'] = row[0].lower()\n",
    "            entry['lemma'] = row[i].lower()\n",
    "            entry['eagle'] = row[i+1].lower()\n",
    "            lexicon.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = set(entry['flexion'] for entry in lexicon)\n",
    "nouns = set(entry['flexion'] for entry in lexicon if entry['eagle'].startswith('n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/aux/stopwords-es.txt') as f:\n",
    "    stop = [w.strip() for w in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "weird = [\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¨', 'Â©', 'Â¬', '\\xad',\n",
    "    'Â°', 'Â±', 'Â³', 'Â´', 'Â·', 'Â¾', 'Â¿', 'Ã—', 'Ì', 'Ìƒ', 'Íœ', 'Í¡', 'à¹', '\\u200b', 'â€’', 'â€“', 'â€”', 'â€•', 'â€¢',\n",
    "    'â€¦', '\\u202a', 'â€»', 'â‚¬', 'â„¢', 'â†‘', 'â†’', 'â†“', 'â†˜', 'â†™', 'âˆ†', 'âˆš', 'âˆž', 'âˆ´', 'âˆµ', 'âŠ™', 'â•¥', 'â–',\n",
    "    'â–‚', 'â–ª', 'â–¬', 'â–¶', 'â—€', 'â—†', 'â—‡', 'â—‹', 'â—', 'â—”', 'â—¡', 'â˜€', 'â˜…', 'â˜†', 'â˜º', 'â˜»', 'â™ ', 'â™¡', 'â™¢',\n",
    "    'â™£', 'â™¤', 'â™¥', 'â™¦', 'â™§', 'âšœ', 'âš«', 'âœ…', 'âœ‹', 'âœŒ', 'âœŽ', 'âœ”', 'âœ°', 'âœ¿', 'âŽ', 'â“', 'â', 'âž', 'â¤',\n",
    "    'â¥', 'â¬†', 'â¬‡', 'â­•', 'ã€Š', 'ã€‹', 'ï¸', 'ï¸¿', 'ï¹', '\\ufeff', 'ï½¡', 'ï¿½', 'ðŸŒˆ', 'ðŸŒŠ', 'ðŸŒš', 'ðŸŒŸ', 'ðŸŒ·',\n",
    "    'ðŸŒ¸', 'ðŸŒ¹', 'ðŸŒ¼', 'ðŸŽ', 'ðŸŽ„', 'ðŸŽ‰', 'ðŸŽŠ', 'ðŸŽ¶', 'ðŸ„', 'ðŸ»', 'ðŸ¼', 'ðŸ½', 'ðŸº', 'ðŸ»', 'ðŸ‘€', 'ðŸ‘‰', 'ðŸ‘Š',\n",
    "    'ðŸ‘‹', 'ðŸ‘Œ', 'ðŸ‘', 'ðŸ‘', 'ðŸ‘‘', 'ðŸ‘»', 'ðŸ’', 'ðŸ’‹', 'ðŸ’•', 'ðŸ’–', 'ðŸ’˜', 'ðŸ’™', 'ðŸ’š', 'ðŸ’›', 'ðŸ’œ', 'ðŸ’ž', 'ðŸ’ ',\n",
    "    'ðŸ’©', 'ðŸ“–', 'ðŸ“š', 'ðŸ”¥', 'ðŸ”«', 'ðŸ–Š', 'ðŸ–', 'ðŸ–•', 'ðŸ˜€', 'ðŸ˜', 'ðŸ˜‚', 'ðŸ˜ƒ', 'ðŸ˜„', 'ðŸ˜…', 'ðŸ˜†', 'ðŸ˜‡', 'ðŸ˜ˆ',\n",
    "    'ðŸ˜‰', 'ðŸ˜Š', 'ðŸ˜‹', 'ðŸ˜Œ', 'ðŸ˜', 'ðŸ˜Ž', 'ðŸ˜', 'ðŸ˜', 'ðŸ˜“', 'ðŸ˜”', 'ðŸ˜•', 'ðŸ˜–', 'ðŸ˜˜', 'ðŸ˜™', 'ðŸ˜š', 'ðŸ˜œ', 'ðŸ˜',\n",
    "    'ðŸ˜Ÿ', 'ðŸ˜¢', 'ðŸ˜£', 'ðŸ˜¥', 'ðŸ˜¨', 'ðŸ˜«', 'ðŸ˜¬', 'ðŸ˜­', 'ðŸ˜°', 'ðŸ˜±', 'ðŸ˜³', 'ðŸ˜µ', 'ðŸ˜·', 'ðŸ˜¼', 'ðŸ™‚', 'ðŸ™„', 'ðŸ™‡',\n",
    "    'ðŸ™ˆ', 'ðŸ™Š', 'ðŸ™', 'ðŸ¤“', 'ðŸ¤—'\n",
    "]\n",
    "others = punctuation + weird\n",
    "# adverbs = [\n",
    "#     'ahora', 'antes', 'despuÃ©s', 'tarde', 'luego', 'ayer', 'temprano', 'ya', 'todavÃ­a', 'anteayer', 'aÃºn',\n",
    "#     'pronto', 'hoy', 'aquÃ­', 'ahÃ­', 'allÃ­', 'cerca', 'lejos', 'fuera', 'dentro', 'alrededor', 'aparte',\n",
    "#     'encima', 'debajo', 'delante', 'detrÃ¡s', 'asÃ­', 'bien', 'mal', 'despacio', 'deprisa', 'como', 'mucho',\n",
    "#     'poco', 'muy', 'casi', 'todo', 'nada', 'algo', 'medio', 'demasiado', 'bastante', 'mÃ¡s', 'menos', 'ademÃ¡s',\n",
    "#     'incluso', 'tambiÃ©n', 'sÃ­', 'tambiÃ©n', 'asimismo', 'no', 'tampoco', 'jamÃ¡s', 'nunca', 'acaso', 'quizÃ¡',\n",
    "#     'quizÃ¡s', 'tal', 'vez', 'tan'\n",
    "# ]\n",
    "# prepositions = [\n",
    "#     'a', 'ante', 'bajo', 'cabe', 'con', 'contra', 'de', 'desde', 'en', 'entre', 'hacia', 'hasta', 'para',\n",
    "#     'por', 'segÃºn', 'sin', 'so', 'sobre', 'tras', 'durante', 'mediante', 'excepto', 'salvo', 'incluso',\n",
    "#     'mÃ¡s', 'menos',\n",
    "# ]\n",
    "stop += stopwords.words('english') + stopwords.words('spanish') + others # adverbs + prepositions\n",
    "\n",
    "my_own_stop_words = ['dije']\n",
    "stop += my_own_stop_words\n",
    "\n",
    "def remove_accent_marks(s):\n",
    "    return s.replace('Ã¡', 'a').replace('Ã©', 'e').replace('Ã­', 'i').replace('Ã³', 'o').replace('Ãº', 'u')\n",
    "\n",
    "stop += [remove_accent_marks(w) for w in stop]\n",
    "\n",
    "def clean(s, only_nouns=True, no_accent_marks=True):\n",
    "    r = s.lower()\n",
    "    for p in others:\n",
    "        r = r.replace(p, '')\n",
    "    words = [w for w in nltk.word_tokenize(r) if w not in stop and not w.isnumeric() and len(w) > 1]\n",
    "    if only_nouns:\n",
    "        words = [w for w in words if w in nouns or w not in all_words]\n",
    "    r = ' '.join(words)\n",
    "    if no_accent_marks:\n",
    "        r = remove_accent_marks(r)\n",
    "    return r\n",
    "\n",
    "def join_texts(d):\n",
    "    return ' '.join(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 40 ms, total: 2min 29s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for story in stories:\n",
    "    story['texts'] = dict((k, clean(v)) for k, v in story['texts'].items())\n",
    "    story['text'] = join_texts(story['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/out/clean_stories.pickle', 'wb') as f:\n",
    "    pickle.dump(stories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
